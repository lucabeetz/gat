{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/.miniconda3/envs/gat/lib/python3.10/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import optax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "from tqdm import tqdm\n",
    "from gat import GAT\n",
    "from data import get_cora_dataset\n",
    "from train import Batch, TrainingState\n",
    "\n",
    "# Training config\n",
    "SEED = 42\n",
    "NUM_CLASSES = 7\n",
    "MAX_STEPS = 1000\n",
    "PATIENCE = 100\n",
    "\n",
    "# GAT config\n",
    "GAT_LAYERS = 2\n",
    "GAT_HEADS = [8, 1]\n",
    "GAT_FEATURES = [8, NUM_CLASSES]\n",
    "DROPOUT = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gat_fn(batch: Batch, is_training: bool) -> jnp.ndarray:\n",
    "    gat = GAT(\n",
    "        num_layers=2,\n",
    "        num_heads=[8, 1],\n",
    "        num_features=[8, NUM_CLASSES],\n",
    "        dropout=0.6\n",
    "    )\n",
    "\n",
    "    return gat(batch.nodes_features, batch.connectivity_mask, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = hk.transform(gat_fn)\n",
    "optimiser = optax.adam(5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params: hk.Params, batch: Batch, rng) -> jnp.ndarray:\n",
    "    \"\"\"Cross-entropy classification loss, regularised by L2 weight decay\"\"\"\n",
    "    batch_size = len(batch.node_indices)\n",
    "    logits = gat.apply(params, rng, batch, is_training=True)[0].take(batch.node_indices, axis=0)\n",
    "    targets = jax.nn.one_hot(batch.labels.take(batch.node_indices), NUM_CLASSES)\n",
    "\n",
    "    log_likelihood = jnp.sum(targets * jax.nn.log_softmax(logits, axis=-1))\n",
    "    l2_regulariser = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "    return -log_likelihood / batch_size + 5e-4 * l2_regulariser\n",
    "\n",
    "@jax.jit\n",
    "def update(state: TrainingState, batch: Batch):\n",
    "    rng, new_rng = jax.random.split(state.rng)\n",
    "    loss_and_grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = loss_and_grad_fn(state.params, batch, rng)\n",
    "\n",
    "    updates, new_opt_state = optimiser.update(grads, state.opt_state)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "\n",
    "    new_state = TrainingState(new_params, new_opt_state, new_rng)\n",
    "\n",
    "    return new_state, loss\n",
    "\n",
    "@jax.jit\n",
    "def init(rng: jnp.ndarray, batch: Batch) -> TrainingState:\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    initial_params = gat.init(init_rng, batch, is_training=True)\n",
    "    initial_opt_state = optimiser.init(initial_params)\n",
    "\n",
    "    return TrainingState(\n",
    "        params=initial_params,\n",
    "        opt_state=initial_opt_state,\n",
    "        rng=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and create batches\n",
    "nodes_features, connectivity_mask, labels, train_indices, val_indices, test_indices = get_cora_dataset()\n",
    "\n",
    "train_data = Batch(nodes_features=nodes_features, labels=labels, connectivity_mask=connectivity_mask, node_indices=train_indices)\n",
    "val_data = Batch(nodes_features=nodes_features, labels=labels, connectivity_mask=connectivity_mask, node_indices=val_indices)\n",
    "test_data = Batch(nodes_features=nodes_features, labels=labels, connectivity_mask=connectivity_mask, node_indices=test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/.miniconda3/envs/gat/lib/python3.10/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/Users/luca/.miniconda3/envs/gat/lib/python3.10/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(SEED)\n",
    "state = init(rng, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val acc: 0.825:  21%|██        | 211/1000 [01:18<04:55,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training, best val acc: 0.8401033878326416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "patience_count = 0\n",
    "best_params = None\n",
    "\n",
    "# Training loop with early stopping\n",
    "pbar = tqdm(range(MAX_STEPS))\n",
    "for step in pbar:\n",
    "    # Update parameters on training data\n",
    "    state, loss = update(state, train_data)\n",
    "\n",
    "    # Validate performance and validation data\n",
    "    rng, new_rng = jax.random.split(state.rng)\n",
    "    logits = gat.apply(state.params, rng, val_data, is_training=False)[0]\n",
    "\n",
    "    # Calculate validation accuracy and update state with new rng_key\n",
    "    val_acc = jnp.equal(jnp.argmax(logits, axis=-1), val_data.labels).mean()\n",
    "    state = TrainingState(state.params, state.opt_state, new_rng)\n",
    "\n",
    "    # Update progress bar\n",
    "    pbar.set_description(f'Val acc: {val_acc:.3f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_params = state.params.copy()\n",
    "        patience_count = 0\n",
    "    else:\n",
    "        patience_count += 1\n",
    "\n",
    "    if patience_count > PATIENCE:\n",
    "        print(f'Finished training, best val acc: {best_val_acc:.3f}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/.miniconda3/envs/gat/lib/python3.10/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/Users/luca/.miniconda3/envs/gat/lib/python3.10/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    }
   ],
   "source": [
    "rng, new_rng = jax.random.split(state.rng)\n",
    "test_logits = gat.apply(best_params, rng, test_data, is_training=False)[0]\n",
    "\n",
    "test_acc = jnp.equal(jnp.argmax(test_logits, axis=-1), test_data.labels).mean()\n",
    "state = TrainingState(state.params, state.opt_state, new_rng)\n",
    "\n",
    "print(f'Test accuracy: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2911b54144ede50fd3c0f7a83274a64649a49893b9a153ef08a95230ead62f07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
